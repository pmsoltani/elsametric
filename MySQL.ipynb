{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as mysql\n",
    "import json\n",
    "\n",
    "with open('config.json', 'r') as read_file:\n",
    "    client = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000002358A00D3C8>\n"
     ]
    }
   ],
   "source": [
    "db = mysql.connect(\n",
    "    host = 'localhost',\n",
    "    buffered = True,\n",
    "    user = client['MySQL User'],\n",
    "    passwd = client['MySQL Pass'],\n",
    "    database = 'scopus'\n",
    ")\n",
    "\n",
    "# column names\n",
    "subject_col = ['asjc_code', 'top', 'middle', 'low']\n",
    "source_col = ['source_id_scp', 'title', 'url', 'type', 'issn', 'e_issn', 'isbn', 'publisher', 'country']\n",
    "source_subject_col = ['source_id', 'subject_id']\n",
    "paper_col = ['paper_id_scp', 'eid', 'title', 'type', 'type_description', 'abstract', 'total_author', 'open_access', 'cited_cnt', 'url', 'article_no', 'fund_no', 'retrieval_time', 'source_id', 'doi', 'volume', 'issue', 'date', 'page_range']\n",
    "author_col = ['author_id_scp', 'first', 'last', 'initials', 'sex', 'type', 'rank', 'email',]\n",
    "paper_author_col = ['paper_id', 'author_id', 'author_no']\n",
    "author_profile_col = ['author_id', 'url', 'type']\n",
    "department_col = ['department_id', 'name', 'abbreviation', 'type', 'lat', 'lng']\n",
    "author_department_col = ['author_id', 'department_id']\n",
    "institution_col = ['institution_id_scp', 'name', 'abbreviation', 'city', 'country', 'url', 'type', 'lat', 'lng']\n",
    "department_institution_col = ['department_id', 'institution_id']\n",
    "\n",
    "subject_q = '''INSERT INTO subject (asjc_code, top, middle, low) VALUES (%s, %s, %s, %s)'''\n",
    "source_q = '''INSERT INTO source (source_id_scp, title, url, type, issn, e_issn, isbn, publisher, country) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "source_subject_q = '''INSERT INTO source_subject (source_id, subject_id) VALUES (%s, %s)'''\n",
    "paper_q = '''INSERT INTO paper (paper_id_scp, eid, title, type, type_description, abstract, total_author, open_access, cited_cnt, url, article_no, fund_no, retrieval_time, source_id, doi, volume, issue, date, page_range) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "author_q = '''INSERT INTO author (author_id_scp, first, last, initials, sex, type, rank, email) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "paper_author_q = '''INSERT INTO paper_author (paper_id, author_id, author_no) VALUES (%s, %s, %s)'''\n",
    "author_profile_q = '''INSERT INTO author_profile (author_id, url, type) VALUES (%s, %s, %s)'''\n",
    "department_q = '''INSERT INTO department (department_id, name, abbreviation, type, lat, lng) VALUES (%s, %s, %s, %s, %s, %s)'''\n",
    "author_department_q = '''INSERT INTO author_department (author_id, department_id) VALUES (%s, %s)'''\n",
    "institution_q = '''INSERT INTO institution (institution_id_scp, name, abbreviation, city, country, url, type, lat, lng) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "department_institution_q = '''INSERT INTO department_institution (department_id, institution_id) VALUES (%s, %s)'''\n",
    "\n",
    "cols = [\n",
    "    subject_col, source_col, source_subject_col, paper_col, author_col, \n",
    "    paper_author_col, author_profile_col, department_col, author_department_col, \n",
    "    institution_col, department_institution_col\n",
    "]\n",
    "\n",
    "col_names = [\n",
    "    'subject', 'source', 'source_subject', 'paper', 'author', \n",
    "    'paper_author', 'author_profile', 'department', 'author_department', \n",
    "    'institution', 'department_institution'\n",
    "]\n",
    "\n",
    "for name, col, que in zip(col_names, cols, ques):\n",
    "    table_name = name\n",
    "    table_col = col\n",
    "    q = f'INSERT INTO {table_name} ({\", \".join(table_col)}) VALUES ({\"%s, \" * (len(table_col) - 1)}%s)'\n",
    "\n",
    "print(db)\n",
    "cursor = db.cursor()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "\n",
    "import os\n",
    "import io\n",
    "import csv\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "path = 'data\\\\Sharif University of Technology'\n",
    "files = list(os.walk(path))[0][2]\n",
    "\n",
    "with io.open(os.path.join(path, files[0]), 'r', encoding='utf8') as raw:\n",
    "    data = json.load(raw)\n",
    "\n",
    "faculties = []\n",
    "with io.open('data\\\\faculties.csv', 'r', encoding='utf-8-sig') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    for row in reader:\n",
    "        if row['Scopus']:\n",
    "            row['Scopus'] = list(map(int, row['Scopus'].split(',')))\n",
    "        faculties.append(row)\n",
    "\n",
    "asjc = []\n",
    "with io.open('data\\\\ASJC Codes.csv', 'r', encoding='utf-8-sig') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    for row in reader:\n",
    "        asjc.append(row)\n",
    "\n",
    "sources = []\n",
    "with io.open('data\\\\Scopus Sources.csv', 'r', encoding='utf-8-sig') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    for row in reader:\n",
    "        row.pop('Active', None)\n",
    "        row.pop('Discontinued', None)\n",
    "        row.pop('Coverage', None)\n",
    "        row.pop('2016 CiteScore', None)\n",
    "        row.pop('2017 CiteScore', None)\n",
    "        row.pop('2018 CiteScore', None)\n",
    "        row.pop('Medline-sourced', None)\n",
    "        row.pop('Open Access', None)\n",
    "        row.pop('Articles in Press Included', None)\n",
    "        row.pop('Added to list April 2019', None)\n",
    "        row.pop('Title history indication', None)\n",
    "        row.pop('Related title to title history indication', None)\n",
    "        row.pop('Other related title 1', None)\n",
    "        row.pop('Other related title 2', None)\n",
    "        row.pop('Other related title 3', None)\n",
    "        row.pop('Publisher imprints grouped to main Publisher', None)\n",
    "        \n",
    "        row['ASJC'] = [int(code) for code in row['ASJC'].split(';') if code != '']\n",
    "        sources.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "#     print(file)\n",
    "    year = file.split('.')[0].split('_')[-4][1:]\n",
    "    with io.open(os.path.join(path, file), 'r', encoding='utf8') as raw:\n",
    "        data = json.load(raw)\n",
    "    data = data['search-results']['entry']\n",
    "    for paper in data:\n",
    "        print(int(paper['dc:identifier'].split(':')[1]))\n",
    "        rnd_source = random.randint(100000,200000)\n",
    "        source_info = [\n",
    "            (int(paper['source-id']) if 'source-id' in paper.keys() else rnd_source),\n",
    "            (paper['prism:publicationName'] if 'prism:publicationName' in paper.keys() else 'No Name!'),\n",
    "            'https://www.scopus.com/sourceid/', # url\n",
    "            (paper['prism:issn'] if 'prism:issn' in paper.keys() else None),\n",
    "            (paper['prism:isbn'][0]['$'] if 'prism:isbn' in paper.keys() else None),\n",
    "            None, # **subject\n",
    "            None, # **publisher\n",
    "            (paper['prism:aggregationType'] if 'prism:aggregationType' in paper.keys() else None),\n",
    "        ]\n",
    "        cursor.execute(source_q, source_info)\n",
    "        \n",
    "        paper_info = [\n",
    "            int(paper['dc:identifier'].split(':')[1]),\n",
    "            paper['eid'],\n",
    "            paper['dc:title'],\n",
    "            (paper['subtype'] if 'subtype' in paper.keys() else None),\n",
    "            (paper['subtypeDescription'] if 'subtypeDescription' in paper.keys() else None),\n",
    "            (paper['dc:description'] if 'dc:description' in paper.keys() else None),\n",
    "            paper['author-count']['$'],\n",
    "            (paper['openaccess'] if 'openaccess' in paper.keys() else None),\n",
    "            paper['citedby-count'],\n",
    "            paper['link'][-2]['@href'],\n",
    "            (paper['article-number'] if 'article-number' in paper.keys() else None),\n",
    "            (paper['fund-no'] if 'fund-no' in paper.keys() else None),\n",
    "            datetime.utcfromtimestamp(int(file.split('.')[0].split('_')[-1])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            (int(paper['source-id']) if 'source-id' in paper.keys() else rnd_source),\n",
    "            (paper['prism:doi'] if 'prism:doi' in paper.keys() else None),\n",
    "            (paper['prism:volume'] if 'prism:volume' in paper.keys() else None),\n",
    "            (paper['prism:issueIdentifier'] if 'prism:issueIdentifier' in paper.keys() else None),\n",
    "            (datetime.strptime(paper['prism:coverDate'], '%Y-%m-%d').strftime('%Y-%m-%d') if 'prism:coverDate' in paper.keys() else year),\n",
    "            (paper['prism:pageRange'] if 'prism:pageRange' in paper.keys() else None),\n",
    "        ]\n",
    "#         cursor.execute(paper_q, paper_info)\n",
    "        \n",
    "        author_info = []\n",
    "        paper_author_info = []\n",
    "        for auth in paper['author']:\n",
    "            author_info.append(\n",
    "                [\n",
    "                    int(auth['authid']),\n",
    "                    auth['given-name'],\n",
    "                    auth['surname'],\n",
    "                    auth['initials'],\n",
    "                    auth['author-url'],\n",
    "                    None, # sex\n",
    "                    None, # type\n",
    "                    None, # rank\n",
    "                    None, # email\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            paper_author_info.append(\n",
    "                [\n",
    "                    paper_info[0],\n",
    "                    int(auth['authid']),\n",
    "                    auth['@seq'],\n",
    "                ]\n",
    "            )\n",
    "        db.commit()\n",
    "db.close()\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self, config: dict, db_name: str, host: str = 'localhost', port: int = 3306, buffered: bool = True):\n",
    "        print('@ __init__')\n",
    "        self._params = {\n",
    "            'host': host,\n",
    "            'buffered': buffered,\n",
    "            'user': config['MySQL User'],\n",
    "            'pass': config['MySQL Pass'],\n",
    "        }\n",
    "        self.db_name = db_name\n",
    "        self.db = None\n",
    "        self.cursor = None\n",
    "        self.tables = []\n",
    "        print('__init__ done!')\n",
    "    \n",
    "    def _connect(self):\n",
    "        print('@ _connect')\n",
    "        if not self.db:\n",
    "            self.db = mysql.connect(\n",
    "                host = self._params['host'],\n",
    "                buffered = self._params['buffered'],\n",
    "                user = self._params['user'],\n",
    "                password = self._params['pass'],\n",
    "                database = self.db_name\n",
    "            )\n",
    "        print('_connect done!')\n",
    "        return self.db\n",
    "    \n",
    "    def _cursor(self):\n",
    "        print('@ _cursor')\n",
    "        if not self.db:\n",
    "            self._connect()\n",
    "        if not self.db.is_connected():\n",
    "            self.db.reconnect()\n",
    "        if not self.cursor:\n",
    "            self.cursor = self.db.cursor()\n",
    "        print('_cursor done!')\n",
    "        return self.cursor\n",
    "    \n",
    "    def _execute(self, query, values = [], fetch: bool = False, many: bool = False, close_cursor: bool = False):\n",
    "        print('@ _execute')\n",
    "        if many:\n",
    "            self._cursor().executemany(query, values)\n",
    "        else:\n",
    "            self._cursor().execute(query, values)\n",
    "        if fetch:\n",
    "            server_response = self.cursor.fetchall()\n",
    "        else:\n",
    "            server_response = self.cursor\n",
    "        # print(server_response)\n",
    "        print('_execute done!')\n",
    "        if close_cursor:\n",
    "            self.cursor.close()\n",
    "            self.cursor = None\n",
    "        return server_response\n",
    "    \n",
    "    def _close(self):\n",
    "        print('@ _close')\n",
    "        if self.db.is_connected():\n",
    "            self.db.close()\n",
    "        print('Closed!')\n",
    "    \n",
    "    def _show_tables(self):\n",
    "        print('@ _show_tables')\n",
    "        return [table[0] for table in self._execute(query = 'SHOW TABLES', fetch = True)]\n",
    "    \n",
    "    def _has_table(self, table_name):\n",
    "        print('@ _has_table')\n",
    "        table_names = self._show_tables()\n",
    "        if table_name in table_names:\n",
    "            print('_has_table done!')\n",
    "            return True\n",
    "        print('_has_table done!')\n",
    "        return False\n",
    "    \n",
    "    def _column_names(self, table_name):\n",
    "        print('@ _column_names')\n",
    "        return [col[0] for col in self.describe(table_name)]\n",
    "    \n",
    "    def _insert(self, table_name, data: list):\n",
    "        print('@ _insert')\n",
    "        if not self._has_table(table_name):\n",
    "            return f'Error! \"{table_name}\" table not found'\n",
    "        column_names = self._column_names\n",
    "        \n",
    "        # assuming all data rows have the same columns\n",
    "        # data is a list of dictionaries, of which the keys are column names\n",
    "        for col in data[0].keys():\n",
    "            if col not in column_names:\n",
    "                return f'Error! \"{col}\" column not found'\n",
    "        \n",
    "        column_names = list(data[0].keys())\n",
    "        query = f'INSERT INTO {table_name} ({\", \".join(column_names)}) VALUES ({\"%s, \" * (len(column_names) - 1)}%s)'\n",
    "        values = []\n",
    "        for row in data:\n",
    "            values.append(tuple(row[col] for col in column_names))\n",
    "        try:\n",
    "            self._execute(query, values, many = True)\n",
    "            print('_insert done!')\n",
    "            return self.db.commit()\n",
    "        except Exception as e:\n",
    "            print(f'error here: {e}')\n",
    "            self._close()\n",
    "    \n",
    "    def describe(self, table_name: str = ''):\n",
    "        print('@ describe')\n",
    "        if table_name:\n",
    "            query = f'DESCRIBE {table_name}'\n",
    "            print('describe done!')\n",
    "            return self._execute(query = query, fetch = True)\n",
    "        server_response = self._show_tables()\n",
    "        for table in server_response:\n",
    "            self.tables.append({table: self.describe(table)})\n",
    "        print('describe done!')\n",
    "        return self.tables\n",
    "    \n",
    "    def _read(self, table_name: str, search: dict, select = '*', result_columns: bool = False):\n",
    "        print('@ _read')\n",
    "        if not self._has_table(table_name):\n",
    "            return f'Error! \"{table_name}\" table not found'\n",
    "        \n",
    "        query = (f'SELECT {select} FROM {table_name} WHERE ' \n",
    "            + ' AND '.join([f'{k} {v[\"operator\"]} {v[\"value\"]}' for k, v in search.items()]))\n",
    "        \n",
    "        server_response = self._execute(query = query, fetch = True, close_cursor = True)\n",
    "        print('got the response from _execute')\n",
    "        if result_columns:\n",
    "            result = []\n",
    "            column_names = self._column_names(table_name)\n",
    "            for row in server_response:\n",
    "                result.append({name: value for name, value in zip(column_names, row)})\n",
    "            print('_read done!')\n",
    "            return result\n",
    "        print('_read done!')\n",
    "        return server_response\n",
    "    \n",
    "    def _has_row(self, table_name, search: dict):\n",
    "        print('@ _has_row')\n",
    "        server_response = self._read(table_name, search, select = 'COUNT(*)')\n",
    "        print('_has_row done!')\n",
    "        return server_response[0][0]\n",
    "    \n",
    "    def _table_order(self):\n",
    "        print('@ _table_order')\n",
    "        return [\n",
    "            {'source', 'subject', 'country', 'paper_funding'},\n",
    "            {'source_subject', 'paper'},\n",
    "            {'author', 'keyword'},\n",
    "            {'paper_author', 'paper_keyword', 'department', 'author_profile'},\n",
    "            {'institution', 'author_department'},\n",
    "            {'department_institution'},\n",
    "        ]\n",
    "    \n",
    "    def db_insert(self, data: list):\n",
    "        print('@ db_insert')\n",
    "        # data is a list of 2-layared dictionaries:\n",
    "        # 1st layer for the table names and 2nd for the column names\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_inspector(data: dict):\n",
    "    warnings = []\n",
    "    top_keys = [\n",
    "        'source-id', 'prism:publicationName', 'prism:coverDate',\n",
    "        'dc:identifier', 'eid', 'dc:title', 'subtype', 'author-count', 'openaccess', 'citedby-count', 'link', \n",
    "        'author', 'affiliation',\n",
    "    ]\n",
    "    author_keys = ['authid', '@seq', 'afid']\n",
    "    affiliation_keys = ['afid', 'affilname']\n",
    "    \n",
    "    keys = data.keys()\n",
    "    for key in top_keys:\n",
    "        if key not in keys:\n",
    "            warnings.append(key)\n",
    "    if 'author' not in warnings:\n",
    "        for author in data['author']:\n",
    "            keys = author.keys()\n",
    "            for key in author_keys:\n",
    "                if key not in keys:\n",
    "                    warnings.append(f'author:{key}')\n",
    "    if 'affiliation' not in warnings:\n",
    "        for affiliation in data['affiliation']:\n",
    "            keys = affiliation.keys()\n",
    "            for key in affiliation_keys:\n",
    "                if key not in keys:\n",
    "                    warnings.append(f'affiliation:{key}')\n",
    "    return warnings\n",
    "\n",
    "paper_info = [\n",
    "    (paper['dc:description'] if 'dc:description' in paper.keys() else None),\n",
    "    paper['author-count']['$'],\n",
    "    (paper['openaccess'] if 'openaccess' in paper.keys() else None),\n",
    "    paper['citedby-count'],\n",
    "    paper['link'][-2]['@href'],\n",
    "    (paper['article-number'] if 'article-number' in paper.keys() else None),\n",
    "    (paper['fund-no'] if 'fund-no' in paper.keys() else None),\n",
    "    datetime.utcfromtimestamp(int(file.split('.')[0].split('_')[-1])).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    (int(paper['source-id']) if 'source-id' in paper.keys() else rnd_source),\n",
    "    (paper['prism:doi'] if 'prism:doi' in paper.keys() else None),\n",
    "    (paper['prism:volume'] if 'prism:volume' in paper.keys() else None),\n",
    "    (paper['prism:issueIdentifier'] if 'prism:issueIdentifier' in paper.keys() else None),\n",
    "    (datetime.strptime(paper['prism:coverDate'], '%Y-%m-%d').strftime('%Y-%m-%d') if 'prism:coverDate' in paper.keys() else year),\n",
    "    (paper['prism:pageRange'] if 'prism:pageRange' in paper.keys() else None),\n",
    "]\n",
    "\n",
    "author_info = []\n",
    "paper_author_info = []\n",
    "for auth in paper['author']:\n",
    "    author_info.append(\n",
    "        [\n",
    "            int(auth['authid']),\n",
    "            auth['given-name'],\n",
    "            auth['surname'],\n",
    "            auth['initials'],\n",
    "            auth['author-url'],\n",
    "            None, # sex\n",
    "            None, # type\n",
    "            None, # rank\n",
    "            None, # email\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    paper_author_info.append(\n",
    "        [\n",
    "            paper_info[0],\n",
    "            int(auth['authid']),\n",
    "            auth['@seq'],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def key_get(data: dict, keys: dict_keys, key: str):\n",
    "    result = (data[key] if key in keys else None)\n",
    "    if type(result) == list:\n",
    "        return result[0]['$']\n",
    "    if type(result) == dict:\n",
    "        return result['$']\n",
    "    return result\n",
    "\n",
    "def data_cleaner(data: dict):\n",
    "    # data is a dictionary containing the info about 1 paper\n",
    "    warnings = data_inspector(data)\n",
    "    if 'openaccess' in warnings:\n",
    "        data['openaccess'] = '0'\n",
    "        warnings.pop('openaccess')\n",
    "    if 'author:afid' in warnings:\n",
    "        warnings.pop('author:afid')\n",
    "    if len(warnings):\n",
    "        return {'warnings': warnings}\n",
    "    \n",
    "    keys = data.keys()\n",
    "    url = ''\n",
    "    for link in data['link']:\n",
    "        if link['@ref'] == 'scopus':\n",
    "            url = link['@href']\n",
    "            break\n",
    "    if not url:\n",
    "        return {'warnings': ['paper url']}\n",
    "    \n",
    "    result = {\n",
    "        'source': {\n",
    "            'source_id_scp': int(data['source-id']), \n",
    "            'title': data['prism:publicationName'], \n",
    "            'url': 'https://www.scopus.com/sourceid/' + data['source-id'], \n",
    "            'type': key_get(data, keys, 'prism:aggregationType'), \n",
    "            'issn': key_get(data, keys, 'prism:issn'), \n",
    "            'e_issn': key_get(data, keys, 'prism:eIssn'), \n",
    "            'isbn': key_get(data, keys, 'prism:isbn'), \n",
    "            'publisher': None, \n",
    "            'country_id': None\n",
    "        },\n",
    "        'paper_funding': {\n",
    "            'agency_id_scp': key_get(data, keys, 'fund-no'), \n",
    "            'agency':  key_get(data, keys, 'fund-sponsor'), \n",
    "            'agency_acronym': key_get(data, keys, 'fund-acr'), \n",
    "        },\n",
    "        'paper': {\n",
    "            'url': url,\n",
    "            'paper_id_scp': int(data['dc:identifier'].split(':')[1]),\n",
    "            'eid': data['eid'],\n",
    "            'title': data['dc:title'],\n",
    "            'type': data['subtype'],\n",
    "            'type_description': key_get(data, keys, 'subtypeDescription'),\n",
    "            'cited_cnt': data['citedby-count'],\n",
    "            'volume': key_get(data, keys, 'prism:volume'),\n",
    "            'issue': key_get(data, keys, 'prism:volume'),\n",
    "            'page_range': key_get(data, keys, 'prism:volume'),\n",
    "            'date': key_get(data, keys, 'prism:volume'),\n",
    "            'doi': key_get(data, keys, 'prism:volume'),\n",
    "            'open_access': data['openaccess'],\n",
    "            'abstract': key_get(data, keys, 'dc:description'),\n",
    "            'article_no': key_get(data, keys, 'prism:volume'),\n",
    "            'agency_id_scp': None,\n",
    "            'total_authors': key_get(data, keys, 'author-count')\n",
    "        },\n",
    "        'author': {\n",
    "            \n",
    "        },\n",
    "        'keyword': {},\n",
    "        'paper_author': {},\n",
    "        'paper_keyword': {},\n",
    "        'department': {},\n",
    "        'author_department': {},\n",
    "        'institution': {},\n",
    "        'department_institution': {},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ _has_row\n",
      "@ _read\n",
      "@ _has_table\n",
      "@ _show_tables\n",
      "@ _execute\n",
      "@ _cursor\n",
      "_cursor done!\n",
      "_execute done!\n",
      "_has_table done!\n",
      "@ _execute\n",
      "@ _cursor\n",
      "_cursor done!\n",
      "_execute done!\n",
      "got the response from _execute\n",
      "_read done!\n",
      "_has_row done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Database(config = client, db_name = 'scopus')\n",
    "# d.insert_first('subject', [{'asjc_code': 12, 'top': 'hi', 'middle': 'hii', 'low': 'hiii'},{'asjc_code': 15, 'top': 'bye', 'middle': 'byee', 'low': 'byeee'}])\n",
    "# d._read('subject', {'asjc_code': {'value': '12', 'operator': '>'}}, result_columns=False)\n",
    "d._has_row('subject', {'asjc_code': {'value': '12', 'operator': '>'}})\n",
    "# d._table_order()\n",
    "# print(d.describe('subject'))\n",
    "# d._close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharif University of Technology_y2018_015_S9J79E_1558880343.txt\n",
      "SCOPUS_ID:85048800068\n",
      "['openaccess']\n",
      "\n",
      "Sharif University of Technology_y2018_016_S9J79E_1558880345.txt\n",
      "SCOPUS_ID:85056004836\n",
      "['author:afid']\n",
      "\n",
      "Sharif University of Technology_y2018_024_S9J79E_1558880364.txt\n",
      "SCOPUS_ID:85041011942\n",
      "['openaccess']\n",
      "\n",
      "Sharif University of Technology_y2018_024_S9J79E_1558880364.txt\n",
      "SCOPUS_ID:85052450133\n",
      "['dc:title']\n",
      "\n",
      "Sharif University of Technology_y2018_025_S9J79E_1558880366.txt\n",
      "SCOPUS_ID:85052434975\n",
      "['author:afid']\n",
      "\n",
      "Sharif University of Technology_y2018_033_S9J79E_1558880385.txt\n",
      "SCOPUS_ID:85044017205\n",
      "['author:afid']\n",
      "\n",
      "Sharif University of Technology_y2018_060_S9J79E_1558880448.txt\n",
      "SCOPUS_ID:85029503975\n",
      "['source-id']\n",
      "\n",
      "Sharif University of Technology_y2018_063_S9J79E_1558880455.txt\n",
      "SCOPUS_ID:85042594707\n",
      "['author:afid']\n",
      "\n",
      "Sharif University of Technology_y2018_078_S9J79E_1558880491.txt\n",
      "SCOPUS_ID:85062311071\n",
      "['author:afid']\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for file in files:\n",
    "#     print(file)\n",
    "    year = file.split('.')[0].split('_')[-4][1:]\n",
    "    with io.open(os.path.join(path, file), 'r', encoding='utf8') as raw:\n",
    "        data = json.load(raw)\n",
    "    data = data['search-results']['entry']\n",
    "    for paper in data:\n",
    "        warnings = data_inspector(paper)\n",
    "        if warnings:\n",
    "            cnt += 1\n",
    "            print(file)\n",
    "            print(paper['dc:identifier'])\n",
    "            print(warnings)\n",
    "            print('')\n",
    "print(cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
